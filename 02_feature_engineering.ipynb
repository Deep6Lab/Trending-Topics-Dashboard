{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12937497",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook prepares data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4f1368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Year_Scaled</th>\n",
       "      <th>Year_STD</th>\n",
       "      <th>Month</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Abstract_Cleaned</th>\n",
       "      <th>Abstract Length</th>\n",
       "      <th>Keywords_Cleaned</th>\n",
       "      <th>Number of Keywords</th>\n",
       "      <th>Month_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.243352</td>\n",
       "      <td>169.971142</td>\n",
       "      <td>March</td>\n",
       "      <td>['Capital structure', 'Corporate taxation', 'D...</td>\n",
       "      <td>Absent theoretical guidance, empiricists have ...</td>\n",
       "      <td>absent theoret guidance empiricist forc reli u...</td>\n",
       "      <td>1047</td>\n",
       "      <td>['capital structure', 'corporate taxation', 'd...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.243352</td>\n",
       "      <td>169.971142</td>\n",
       "      <td>March</td>\n",
       "      <td>['Credit spreads', 'LBO risk', 'Structural mod...</td>\n",
       "      <td>Recent decades have witnessed several waves of...</td>\n",
       "      <td>recent decad wit sever wave buyout activity fi...</td>\n",
       "      <td>580</td>\n",
       "      <td>['credit spreads', 'lbo risk', 'structural mod...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.243352</td>\n",
       "      <td>169.971142</td>\n",
       "      <td>March</td>\n",
       "      <td>['Fire sales', 'Liquidity management', 'Mutual...</td>\n",
       "      <td>We develop three novel measures of the incenti...</td>\n",
       "      <td>develop three novel measur incent equiti mutua...</td>\n",
       "      <td>586</td>\n",
       "      <td>['fire sales', 'liquidity management', 'mutual...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.243352</td>\n",
       "      <td>169.971142</td>\n",
       "      <td>March</td>\n",
       "      <td>['Asset pricing', 'Leverage constraints', 'Lot...</td>\n",
       "      <td>We test whether the low-risk effect is driven ...</td>\n",
       "      <td>test whether lowrisk effect driven leverag con...</td>\n",
       "      <td>861</td>\n",
       "      <td>['asset pricing', 'leverage constraints', 'lot...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.243352</td>\n",
       "      <td>169.971142</td>\n",
       "      <td>March</td>\n",
       "      <td>['Gender gap', 'Entrepreneurship', 'Angel inve...</td>\n",
       "      <td>We study whether early stage investors have ge...</td>\n",
       "      <td>studi whether earli stage investor gender bias...</td>\n",
       "      <td>742</td>\n",
       "      <td>['gender gap', 'entrepreneurship', 'angel inve...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Year_Scaled    Year_STD  Month  \\\n",
       "0  2020     1.243352  169.971142  March   \n",
       "1  2020     1.243352  169.971142  March   \n",
       "2  2020     1.243352  169.971142  March   \n",
       "3  2020     1.243352  169.971142  March   \n",
       "4  2020     1.243352  169.971142  March   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  ['Capital structure', 'Corporate taxation', 'D...   \n",
       "1  ['Credit spreads', 'LBO risk', 'Structural mod...   \n",
       "2  ['Fire sales', 'Liquidity management', 'Mutual...   \n",
       "3  ['Asset pricing', 'Leverage constraints', 'Lot...   \n",
       "4  ['Gender gap', 'Entrepreneurship', 'Angel inve...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Absent theoretical guidance, empiricists have ...   \n",
       "1  Recent decades have witnessed several waves of...   \n",
       "2  We develop three novel measures of the incenti...   \n",
       "3  We test whether the low-risk effect is driven ...   \n",
       "4  We study whether early stage investors have ge...   \n",
       "\n",
       "                                    Abstract_Cleaned  Abstract Length  \\\n",
       "0  absent theoret guidance empiricist forc reli u...             1047   \n",
       "1  recent decad wit sever wave buyout activity fi...              580   \n",
       "2  develop three novel measur incent equiti mutua...              586   \n",
       "3  test whether lowrisk effect driven leverag con...              861   \n",
       "4  studi whether earli stage investor gender bias...              742   \n",
       "\n",
       "                                    Keywords_Cleaned  Number of Keywords  \\\n",
       "0  ['capital structure', 'corporate taxation', 'd...                   5   \n",
       "1  ['credit spreads', 'lbo risk', 'structural mod...                   4   \n",
       "2  ['fire sales', 'liquidity management', 'mutual...                   3   \n",
       "3  ['asset pricing', 'leverage constraints', 'lot...                   5   \n",
       "4  ['gender gap', 'entrepreneurship', 'angel inve...                   4   \n",
       "\n",
       "   Month_Cleaned  \n",
       "0              3  \n",
       "1              3  \n",
       "2              3  \n",
       "3              3  \n",
       "4              3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load pre-processed data\n",
    "df = pd.read_csv('data/data_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f92e1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05067446,  0.08134886, -0.0360181 , ..., -0.04204805,\n",
       "        -0.05446644,  0.01243352],\n",
       "       [ 0.01195597,  0.02918805,  0.00483241, ...,  0.01169018,\n",
       "         0.00095615,  0.01243352],\n",
       "       [ 0.08326695,  0.19427712, -0.16412329, ..., -0.00405454,\n",
       "         0.17869968,  0.01243352],\n",
       "       ...,\n",
       "       [ 0.62136314, -0.15234657,  0.02324684, ...,  0.00152473,\n",
       "        -0.02040188, -0.02627929],\n",
       "       [ 0.08388964,  0.17353293, -0.28466068, ..., -0.09554684,\n",
       "        -0.12300539, -0.02627929],\n",
       "       [ 0.24049408,  0.21700294, -0.14542568, ...,  0.20482033,\n",
       "         0.2025117 , -0.02627929]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "# build a data pipeline\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),                                 # bag-of-words\n",
    "                     ('lda', LatentDirichletAllocation(n_components=50,           # topic modeling\n",
    "                                                       random_state=42,\n",
    "                                                       topic_word_prior=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svd', TruncatedSVD(n_components=10, random_state=42)),     # components reduction\n",
    "                     ('to_dense', DenseTransformer())])                           # data transform\n",
    "\n",
    "# build data vector for abstract\n",
    "x_vector = pipeline.fit_transform(df.Abstract_Cleaned)\n",
    "\n",
    "# build biased data vector\n",
    "x_vector_bias = np.zeros((x_vector.shape[0], x_vector.shape[1]+1))\n",
    "x_vector_bias[:,-1] = df.Year_Scaled / 100\n",
    "x_vector_bias[:,:-1] = x_vector\n",
    "\n",
    "# display x_vector_bias\n",
    "x_vector_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3faff8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38622589,  0.6200172 , -0.27451943, ..., -0.1636667 ,\n",
       "        -0.32047798, -0.41512727],\n",
       "       [ 0.32305035,  0.78866086,  0.1305716 , ..., -0.03759962,\n",
       "         0.31586857,  0.02583506],\n",
       "       [ 0.09677003,  0.22578231, -0.19073855, ...,  0.90290512,\n",
       "        -0.00471205,  0.20767874],\n",
       "       ...,\n",
       "       [ 0.96622139, -0.23689933,  0.0361489 , ..., -0.00857008,\n",
       "         0.00237096, -0.03172498],\n",
       "       [ 0.08556933,  0.1770075 , -0.29036031, ...,  0.06048049,\n",
       "        -0.09745993, -0.12546828],\n",
       "       [ 0.39128029,  0.35306056, -0.23660542, ..., -0.06404832,\n",
       "         0.33323964,  0.32948353]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize x_vector\n",
    "x_vector = Normalizer().fit_transform(x_vector)\n",
    "x_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3f41f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22002264,  2.2219857 , -1.01177663, ..., -0.73937831,\n",
       "        -1.06955482, -1.67230182],\n",
       "       [ 1.02046173,  2.82636214,  0.48123842, ..., -0.16985951,\n",
       "         1.05417153,  0.10407414],\n",
       "       [ 0.30568027,  0.80914701, -0.70299143, ...,  4.07895096,\n",
       "        -0.01572586,  0.83661459],\n",
       "       ...,\n",
       "       [ 3.05213089, -0.84898762,  0.13323143, ..., -0.03871609,\n",
       "         0.00791277, -0.12780114],\n",
       "       [ 0.27029913,  0.63435036, -1.07016025, ...,  0.27322576,\n",
       "        -0.32526024, -0.50543736],\n",
       "       [ 1.23598864,  1.26528023, -0.87203966, ..., -0.28934377,\n",
       "         1.11214526,  1.32729393]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second method\n",
    "df_x_vector = pd.DataFrame(x_vector, index=None)\n",
    "df_x_vector = df_x_vector / df_x_vector.std()\n",
    "x_vector = df_x_vector.to_numpy()\n",
    "\n",
    "# display x_vector\n",
    "x_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba81127",
   "metadata": {},
   "source": [
    "### Build Terms Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01ca5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "import json\n",
    "\n",
    "# create TF-IDF transformer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(df.Abstract_Cleaned)\n",
    "terms_sparse_matrix = tfidf.transform(df.Abstract_Cleaned)\n",
    "terms_label = tfidf.get_feature_names()\n",
    "\n",
    "# save data\n",
    "# to load sparse matrix: sparse_matrix = scipy.sparse.load_npz('/tmp/sparse_matrix.npz')\n",
    "scipy.sparse.save_npz('data/terms_sparse_matrix.npz', terms_sparse_matrix)\n",
    "\n",
    "# save term labels\n",
    "with open(\"data/terms_label.txt\", \"w\") as fp:\n",
    "    json.dump(terms_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25165a1",
   "metadata": {},
   "source": [
    "## Save Data for Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e39da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save x_vector\n",
    "with open('data/x_vector.npy', 'wb') as file:\n",
    "    np.save(file, x_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63855814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save x_vector_bias\n",
    "with open('data/x_vector_bias.npy', 'wb') as file:\n",
    "    np.save(file, x_vector_bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
